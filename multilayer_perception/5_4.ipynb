{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can you design other cases where a neural network might exhibit symmetry that needs breaking, besides the permutation symmetry in an MLP's layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In convolutional neural networks (CNNs), weight sharing across different channels or filters can lead to symmetry in feature detection. Breaking this symmetry is important to ensure that different filters specialize in detecting different features, improving the networkâ€™s representational capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Can we initialize all weight parameters in linear regression or in softmax regression to the same value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all weights are initialized to the same value, each neuron will compute the same output, leading to symmetry in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Look up analytic bounds on the eigenvalues of the product of two matrices. What does this tell you about ensuring that gradients are well conditioned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral norm of a matrix A is defined as the largest singular value of A which is equal to the square root of the largest eigenvalue of $A^TA$. When considering the product of two matrices, AB, the spectral norm of the product is bounded by the product of the individual spectral norms: $\\|AB\\|_2 \\leq \\|A\\|_2 \\cdot \\|B\\|_2$\n",
    "\n",
    "The eigenvalues of the product of weight matrices (used in the forward and backward passes) can impact the conditioning of gradients:\n",
    "\n",
    "- Vanishing Gradients: If the eigenvalues of weight matrices are close to zero, gradients may vanish during backpropagation, leading to slow convergence or getting stuck in a poor local minimum.\n",
    "\n",
    "- Exploding Gradients: If the eigenvalues of weight matrices are very large, gradients may explode during backpropagation, causing optimization difficulties and numerical instability.\n",
    "\n",
    "As the eigenvalues are bounded by the spectral norms of the matrix. We can use techniques like weight decay or dropout to controll the magnitude of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. If we know that some terms diverge, can we fix this after the fact? Look at the paper on layerwise adaptive rate scaling for inspiration(You, Y., Gitman, I., & Ginsburg, B. (2017). Large batch training of convolutional networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the mentioned paper, the authors propose a technique called Layer-Wise Adaptive Rate Scaling (LARS) to address gradient divergence and other optimization challenges in training deep neural networks. LARS focuses on controlling the learning rate of each layer based on the local Lipschitzness of the loss with respect to the weights.\n",
    "\n",
    "LARS adjusts the learning rates of individual layers based on their gradient norms and the norms of the weight matrices. It takes into account the ratio of the gradient norms to the weight matrix norms. If this ratio is too large, it indicates that the gradients are becoming unstable, and the learning rates are scaled down to prevent divergence.\n",
    "\n",
    "The key idea is to maintain a balance between the step sizes taken in parameter space and the local curvature of the loss landscape. By adaptively scaling the learning rates based on the gradient-to-weight-norm ratio, LARS helps prevent gradients from exploding and encourages stable training dynamics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
