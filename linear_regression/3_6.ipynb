{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **When can you solve the problem of polynomial regression exactly?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial model can be represented as:\n",
    "\n",
    "$$\\hat{y} = \\sum_{i=0}^d x^iw_i$$\n",
    "\n",
    "So the linear function is:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "x_1^0 & x_1^1 & ... & x_1^d \\\\\n",
    "x_2^0 & x_2^1 & ... & x_2^d \\\\\n",
    " &  & ... &  \\\\\n",
    "x_n^0 & x_n^1 & ... & x_n^d\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w_0 \\\\ w_1 \\\\ ... \\\\ w_d \\end{bmatrix} = \\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ ... \\\\ y_n \\end{bmatrix}$$\n",
    "\n",
    "It's $X\\vec{w}=\\vec{y}$ form. The vector of estimated polynomial regression coefficients (using ordinary least squares estimation) is:\n",
    "\n",
    "$$\\vec{w} = (X^TX)^{-1}X^T\\vec{y}$$\n",
    "\n",
    "The condition for the matrix to be invertible is $n > d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Give at least five examples where dependent random variables make treating the problem as IID data inadvisable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://pandalab.me/archives/generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Can you ever expect to see zero training error? Under which circumstances would you see zero generalization error?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the observations contain noise, a correct model estimate should not get zero training error. If it does, it means the model is overfitting, where the model has captured noise in the data. If the generalization error is zero, it is very likely that examples in the test set have already appeared in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Why is k-fold cross-validation very expensive to compute?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross-validation involves partitioning the dataset into k subsets (or folds), training the model on k−1 folds, and then evaluating its performance on the remaining fold. It is used for selecting model or tuning hyperparameters.\n",
    "\n",
    "So each model or each set of hyperparameters needs k times training. When the search space is large, this way is very computionally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Why is the k-fold cross-validation error estimate biased?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the limited dataset size, the value of k will not be very large. So k times of validation cannot guarantee that the best hyperparameters it selected are the best hyperparameters for the entire data set. It is just the hyperparameters that work best for the specific data splits in the cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **The VC dimension is defined as the maximum number of points that can be classified with arbitrary labels ${\\pm 1}$ by a function of a class of functions. Why might this not be a good idea for measuring how complex the class of functions is? Hint: consider the magnitude of the functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandalab.me/archives/generalization\n",
    "\n",
    "Function Smoothness: The VC dimension doesn’t differentiate between smooth and non-smooth functions. A class of step functions with a high VC dimension can be more complex in terms of shattering points, but they might not generalize well due to their inherent discontinuities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Your manager gives you a difficult dataset on which your current algorithm does not perform so well. How would you justify to him that you need more data? Hint: you cannot increase the data but you can decrease it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandalab.me/archives/generalization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
