{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e11a6c9-7038-4246-a946-de4a9f1dcadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class SyntheticRegressionData(d2l.DataModule):  #@save\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n",
    "                 batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704d773-4096-4917-a179-f341868445f0",
   "metadata": {},
   "source": [
    "1. **What will happen if the number of examples cannot be divided by the batch size. How would you change this behavior by specifying a different argument by using the framework's API?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76aaca-068c-4b20-9364-50d8a71ae38c",
   "metadata": {},
   "source": [
    "The remaining examples will form the last batch whose number is less than the batch size. If we want to drop these samples, we can set the `drop_last` attribute as `True` in `torch.utils.data.DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83282f9e-f7c0-45ca-a541-c4571a15633e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                       shuffle=train, drop_last = True)\n",
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddff10-3790-439d-bb6c-bc68304160aa",
   "metadata": {},
   "source": [
    "2. **Suppose that we want to generate a huge dataset, where both the size of the parameter vector `w` and the number of examples `num_examples` are large.**\n",
    "    1. What happens if we cannot hold all data in memory?\n",
    "    1. How would you shuffle the data if it is held on disk? Your task is to design an *efficient* algorithm that does not require too many random reads or writes. Hint: [pseudorandom permutation generators](https://en.wikipedia.org/wiki/Pseudorandom_permutation) allow you to design a reshuffle without the need to store the permutation table explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efec70-d589-4780-8819-4134b38bbcea",
   "metadata": {},
   "source": [
    "<font color = red>(uncertain)</font>\n",
    "\n",
    "A. We can not shuffle the dataset with a very long list of indices. It will still take too much memory.\n",
    "\n",
    "B. We can generate a pseudorandom permutation by methods such as (Naor, M., & Reingold, O. (1999). On the construction of pseudorandom permutations: Lubyâ€“Rackoff revisited). It can be used to generate indices without storing the whole permutation table. (I didn't study this method carefully)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65e296-1b63-4d7c-b9be-b065012ac1df",
   "metadata": {},
   "source": [
    "3. **Implement a data generator that produces new data on the fly, every time the iterator is called.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab1a06ac-1dec-4deb-abc1-e151e277f8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.6491],\n",
      "        [ 5.3203],\n",
      "        [ 1.4828],\n",
      "        [-2.1962],\n",
      "        [ 1.0877],\n",
      "        [ 0.4515],\n",
      "        [ 2.3016],\n",
      "        [ 5.2006]])\n",
      "tensor([[ 0.6757],\n",
      "        [ 1.5848],\n",
      "        [ 8.9078],\n",
      "        [ 2.7623],\n",
      "        [ 7.8832],\n",
      "        [-7.9047],\n",
      "        [ 9.1790],\n",
      "        [ 2.8393]])\n",
      "tensor([[-0.7600],\n",
      "        [ 2.4155],\n",
      "        [ 3.5540],\n",
      "        [-0.9344],\n",
      "        [ 2.0684],\n",
      "        [11.7490],\n",
      "        [ 4.8920],\n",
      "        [ 7.3684]])\n"
     ]
    }
   ],
   "source": [
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def data_generator(self): \n",
    "    if not hasattr(self, 'iter'):\n",
    "        self.iter = iter(self.train_dataloader())\n",
    "    return next(self.iter)\n",
    "    \n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2, batch_size=8)\n",
    "for i in range(3):\n",
    "    X,y = data.data_generator()\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71dd14-8fbf-4a6e-bbf1-479394b0b0ad",
   "metadata": {},
   "source": [
    "4. **How would you design a random data generator that generates *the same* data each time it is called?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "619a42ae-a942-41c1-90aa-9b6baa553754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0 y= tensor([[ 4.7769],\n",
      "        [ 4.4085],\n",
      "        [10.4552],\n",
      "        [ 5.6137],\n",
      "        [ 9.8315],\n",
      "        [ 6.0097],\n",
      "        [ 1.0880],\n",
      "        [ 1.1387]])\n",
      "Time 1 y= tensor([[ 4.7769],\n",
      "        [ 4.4085],\n",
      "        [10.4552],\n",
      "        [ 5.6137],\n",
      "        [ 9.8315],\n",
      "        [ 6.0097],\n",
      "        [ 1.0880],\n",
      "        [ 1.1387]])\n",
      "Time 2 y= tensor([[ 4.7769],\n",
      "        [ 4.4085],\n",
      "        [10.4552],\n",
      "        [ 5.6137],\n",
      "        [ 9.8315],\n",
      "        [ 6.0097],\n",
      "        [ 1.0880],\n",
      "        [ 1.1387]])\n"
     ]
    }
   ],
   "source": [
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    torch.manual_seed(2) # set the random seed\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2, batch_size=8)\n",
    "for i in range(3):\n",
    "    X, y = next(iter(data.train_dataloader()))\n",
    "    print(\"Time\", i, \"y=\", y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
