{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1208cf-899e-448e-be0f-aa6115422286",
   "metadata": {},
   "source": [
    "1. **Prove that the transpose of the transpose of a matrix is the matrix itself: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbde440-6342-4c88-a1dc-6e9560d00117",
   "metadata": {},
   "source": [
    "See the [link](https://mathinstructor.net/2012/04/we-have-matrix-a-how-to-prove-that-transpose-of-a-transpose-is-equal-to-matrix-a-i-e-att-a/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f85e05-b62e-480d-826a-a649822bcb47",
   "metadata": {},
   "source": [
    "2. **Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that sum and transposition commute:** $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ac7b0-bf2c-4d1c-a2c9-5c1b7a7089db",
   "metadata": {},
   "source": [
    "Suppose the size of A and B is (m,n), then the size of $A^T+B^T$ is (n,m). The size of $(A+B)^T$ is (n,m). The (i,j)-entry of $A^T+B^T$ is the sum of (i,j)-entries of $A^T$ and $B^T$, which are (j,i)-entries of A and B, respectively. Thus the (i,j)-entry of $A^T+B^T$ is the (j,i)-entry of the sum of A and B, which is equal to the (i,j)-entry of the transpose $(A+B)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf1224f-c007-4da6-8bff-1dac60140d88",
   "metadata": {},
   "source": [
    "3. **Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Can you prove the result by using only the results of the previous two exercises?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fada9-b33e-4b6b-93ad-b1deaac8e573",
   "metadata": {},
   "source": [
    "$(A+A^T)^T=A^T+(A^T)^T=A^T+A$. So it is always symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fded668-d161-4b23-9f98-efa49250157e",
   "metadata": {},
   "source": [
    "4. **We defined the tensor `X` of shape (2, 3, 4) in this section. What is the output of `len(X)`? Write your answer without implementing any code, then check your answer using code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378cd6c0-4bf8-4e52-b951-180ba50418bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412db31-dddc-4a51-9095-a3a2f56f5026",
   "metadata": {},
   "source": [
    "5. **For a tensor `X` of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of `X`? What is that axis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c37eb-2751-49b0-941b-ab861659a903",
   "metadata": {},
   "source": [
    "len(X) is the size of the first dimention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ffc1b-7b99-43e9-9a9a-008b479c5f3c",
   "metadata": {},
   "source": [
    "6. **Run `A / A.sum(axis=1)` and see what happens. Can you analyze the results?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f675e4-e08e-49a1-a94f-8f5db8bac5ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m6\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A / A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fce95-6322-401b-98df-49be2b0e11d0",
   "metadata": {},
   "source": [
    "The shape of A is (2, 3). The shape of A.sum(axis=1) is 2. So it can't implement broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6be9a9-5dfd-4305-9acd-c3bfb4144c88",
   "metadata": {},
   "source": [
    "7. **When traveling between two points in downtown Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a94cc0-85cb-46f5-87d6-d46e945af4cd",
   "metadata": {},
   "source": [
    "The distance is the sum of the length of all avenues passing through. Can't travel diagonally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282f349-b0ac-4359-a156-dc89c8d47fb9",
   "metadata": {},
   "source": [
    "8. **Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs along axes 0, 1, and 2?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61246fe4-b9ac-4bcf-bb62-3eedc246a36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([2, 4]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X.sum(axis=0).shape, X.sum(axis=1).shape, X.sum(axis=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a17e2-84dc-4eaa-9c4c-3c5714fc33eb",
   "metadata": {},
   "source": [
    "9. **Feed a tensor with three or more axes to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72c5dd19-9469-4704-9ee1-a50ba1f6b8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = torch.ones((1,3,3))\n",
    "np.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844e5e9-2fbc-4306-a670-29cfdbb47011",
   "metadata": {},
   "source": [
    "The linalg.norm function can compute the norm of a matrix of a vector. When given no parameters, the return of a matrix is the Frobenius norm: $(\\sum_{i,j} (a_{i,j}^2))^{\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4810447-c4e8-4100-9fde-7dd6f16eb32b",
   "metadata": {},
   "source": [
    "10. **Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{14}}$, initialized with Gaussian random variables. You want to compute the product $\\mathbf{A} \\mathbf{B} \\mathbf{C}$. Is there any difference in memory footprint and speed, depending on whether you compute $(\\mathbf{A} \\mathbf{B}) \\mathbf{C}$ or $\\mathbf{A} (\\mathbf{B} \\mathbf{C})$. Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e1c66-c92d-495c-bafc-9dc869066dd2",
   "metadata": {},
   "source": [
    "For matrix A(a,b) and matrix B(b,c), the product AB needs $abc$ multiplications and $a(b-1)c \\approx abc$ additions.\n",
    "\n",
    "- (AB)C:\n",
    "\n",
    "AB: $2^{10} * 2^{16} * 2*5 = 2^{31}$ multiplications/addtions\n",
    "\n",
    "(AB)C: $2^{10} * 2^5 * 2*{14} = 2^{29}$ multiplications/addtions\n",
    "\n",
    "calculation: about $2^{31}$ multiplications/addtions\n",
    "\n",
    "memory: intermediate variable ($2^{10},2^{5}$)\n",
    "\n",
    "- A(BC):\n",
    "\n",
    "BC: $2^{16} * 2^{5} * 2*{14} = 2^{35}$ multiplications/addtions\n",
    "\n",
    "A(BC): $2^{10} * 2^{16} * 2*{14} = 2^{40}$ multiplications/addtions\n",
    "\n",
    "calculation: about $2^{40}$ multiplications/addtions\n",
    "\n",
    "memory: intermediate variable ($2^{16},2^{14}$)\n",
    "\n",
    "So computing (AB)C needs less calculations and memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6301f-d882-4ad3-b051-860eb27cfcff",
   "metadata": {},
   "source": [
    "11. **Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}}$. Is there any difference in speed depending on whether you compute $\\mathbf{A} \\mathbf{B}$ or $\\mathbf{A} \\mathbf{C}^\\top$? Why? What changes if you initialize $\\mathbf{C} = \\mathbf{B}^\\top$ without cloning memory? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a9d173d-5cd9-49e5-996b-f06316511dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A@B 0.01600050926208496\n",
      "2 A@C^T 0.01799941062927246\n",
      "3 A@(B^T)^T 0.021985530853271484\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "A = torch.randn((2 ** 10,2 ** 16))\n",
    "B = torch.randn((2 ** 16,2 ** 5))\n",
    "C = torch.randn((2 ** 5,2 ** 16))\n",
    "\n",
    "time1 = time.time()\n",
    "D = A@B\n",
    "time2 = time.time()\n",
    "print(\"1 A@B\", time2-time1)\n",
    "\n",
    "time1 = time.time()\n",
    "D = A@(C.transpose(0,1))\n",
    "time2 = time.time()\n",
    "print(\"2 A@C^T\",time2-time1)\n",
    "\n",
    "C = B.T\n",
    "time1 = time.time()\n",
    "D = A@(C.transpose(0,1))\n",
    "time2 = time.time()\n",
    "print(\"3 A@(B^T)^T\", time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4c687-51b9-41e9-857a-04686be63644",
   "metadata": {},
   "source": [
    "<font color = green>(from the discussion)</font>\n",
    "<font color = red>(uncertain)</font>\n",
    "\n",
    "$ \\mathbf{A} \\mathbf{C}^{T} $ yields better performance due to the layout of data in memory : since the row major format in which data is usually stored in torch usually prefers memory accesses of the same row, when you take transpose of $ \\mathbf{C}^{\\top} $, it’s not really taking a physical transpose, but a logical one, meaning when we index the trasposes matrix at $ (i, j) $, it just gets internally converted to $ (j, i) $ of the matrix before transposition. Since the elements of the second matrix are accessed column-wise, it is inefficient for this task, but if we have it logically transposed, then the accesses become efficent again, since the data is logically being accessed across rows, ie, columns-wise, but is physically geting accessed across columns, ie, row-wise, since we didn’t actually perform the element swaps, only decided to change indexing under the hood. Hence, the transpose technique works faster.\n",
    "\n",
    "I think the analysis makes sense. So the result should be 2 faster than 1 and 3. But the actual result is that the three methods take about the same amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2736e-df52-4963-8501-dc58e58f3bcf",
   "metadata": {},
   "source": [
    "12. **Consider three matrices, say $\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{100 \\times 200}$. Construct a tensor with three axes by stacking $[\\mathbf{A}, \\mathbf{B}, \\mathbf{C}]$. What is the dimensionality? Slice out the second coordinate of the third axis to recover $\\mathbf{B}$. Check that your answer is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707472c6-db63-4168-9813-1ec4db11347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 200, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn((100,200))\n",
    "B = torch.randn((100,200))\n",
    "C = torch.randn((100,200))\n",
    "D = torch.stack((A, B, C), dim=2)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8ba56b-a50f-4944-a978-9120d4c30c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = D[:,:,1]\n",
    "E.equal(B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
