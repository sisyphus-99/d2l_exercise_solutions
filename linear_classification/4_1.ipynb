{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can explore the connection between exponential families and softmax in some more depth\n",
    "   \n",
    "   (1) Compute the second derivative of the cross-entropy loss $l(\\mathbf{y},\\hat{\\mathbf{y}})$ for softmax.\n",
    "   \n",
    "   (2) Compute the variance of the distribution given by $\\mathrm{softmax}(\\mathbf{o})$ and show that it matches the second derivative computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "\n",
    "$$l(\\mathbf{y},\\hat{\\mathbf{y}}) =\\log\\sum_{k=1}^q\\exp(o_k)-\\sum_{j=1}^qy_jo_j.$$\n",
    "\n",
    "$$\\partial_{o_j}l(\\mathbf{y},\\hat{\\mathbf{y}})=\\frac{\\exp(o_j)}{\\sum_{k=1}^q\\exp(o_k)}-y_j=\\mathrm{softmax}(\\mathbf{o})_j-y_j.$$\n",
    "\n",
    "$$\\frac{\\partial^2 l(\\mathbf{y},\\hat{\\mathbf{y}})}{\\partial o_j^2}=\\frac{\\exp(o_j)\\sum_{k=1}^q\\exp(o_k)-(\\exp(o_j))^2}{(\\sum_{k=1}^q\\exp(o_k))^2}=\\mathrm{softmax}(\\mathbf{o})_j - (\\mathrm{softmax}(\\mathbf{o})_j)^2$$\n",
    "\n",
    "(2)\n",
    "\n",
    "Suppose $\\mathbf{y}$ is a one-hot vector. $P(\\mathbf{y}_i=1)=\\mathrm{softmax}(\\mathbf{o})_j, P(\\mathbf{y}_i=0)=1-\\mathrm{softmax}(\\mathbf{o})_j$. Then $\\mathbf{y}$ follows the distribution given by $\\mathrm{softmax}(\\mathbf{o})$.\n",
    "\n",
    "$$E(\\mathbf{y}_j)=\\mathrm{softmax}(\\mathbf{o}_j)$$\n",
    "\n",
    "$$E(\\mathbf{y}_j^2)=\\mathrm{softmax}(\\mathbf{o}_j)$$\n",
    "\n",
    "$$Var(\\mathbf{y}_j)=\\mathrm{softmax}(\\mathbf{o})_j - (\\mathrm{softmax}(\\mathbf{o})_j)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assume that we have three classes which occur with equal probability, i.e., the probability vector is $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$.\n",
    "    1. What is the problem if we try to design a binary code for it?\n",
    "    2. Can you design a better code? Hint: what happens if we try to encode two independent observations? What if we encode $n$ observations jointly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some people say the problem is the distances between each binary code. But this relates to the definition of the loss function and has nothing to do with the hint. \n",
    "\n",
    "Some people say that the lengths of binary codes are different, which seems to be related to the hint, but I can't understand why the length of binary code(00, 01, 10) are different. https://pandalab.me/archives/softmax_regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. When encoding signals transmitted over a physical wire, engineers do not always use binary codes. For instance, [PAM-3](https://en.wikipedia.org/wiki/Ternary_signal) uses three signal levels $\\{-1, 0, 1\\}$ as opposed to two levels $\\{0, 1\\}$. How many ternary units do you need to transmit an integer in the range $\\{0, \\ldots, 7\\}$? Why might this be a better idea in terms of electronics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transmit an integer in the range $\\{0,...,7\\}$, we need $\\lceil log_3(8) \\rceil=2$\n",
    "\n",
    "For electronics, there are three conditions: positive voltage, negative voltage, zero voltage in a physical wire which can be easily used as three-level ternary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The [Bradley--Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) uses\n",
    "a logistic model to capture preferences. For a user to choose between apples and oranges one\n",
    "assumes scores $o_{\\textrm{apple}}$ and $o_{\\textrm{orange}}$. Our requirements are that larger scores should lead to a higher likelihood in choosing the associated item and that\n",
    "the item with the largest score is the most likely one to be chosen.\n",
    "    1. Prove that softmax satisfies this requirement.\n",
    "    2. What happens if you want to allow for a default option of choosing neither apples nor oranges? Hint: now the user has three choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) \n",
    "   \n",
    "$$P(orange) = \\frac{e^{o_{orange}}}{e^{o_{orange}}+e^{o_{apple}}}$$\n",
    "$$P(apple) = \\frac{e^{o_{apple}}}{e^{o_{orange}}+e^{o_{apple}}}$$\n",
    "\n",
    "We can see that larger scores lead to higher likelihood. \n",
    "\n",
    "(2)\n",
    "\n",
    "Add an output $o_{default}$, $p = softmax(o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Softmax gets its name from the following mapping: $\\textrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b))$.\n",
    "    1. Prove that $\\textrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$.\n",
    "    2. How small can you make the difference between both functions? Hint: without loss of\n",
    "    generality you can set $b = 0$ and $a \\geq b$.\n",
    "    3. Prove that this holds for $\\lambda^{-1} \\textrm{RealSoftMax}(\\lambda a, \\lambda b)$, provided that $\\lambda > 0$.\n",
    "    4. Show that for $\\lambda \\to \\infty$ we have $\\lambda^{-1} \\textrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$.\n",
    "    5. Construct an analogous softmin function.\n",
    "    6. Extend this to more than two numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "\n",
    "Suppose $a \\geq b$:\n",
    "\n",
    "$$log(exp(a)+exp(b)) \\geq log(exp(a)) = a = max(a, b)$$\n",
    "\n",
    "(2)\n",
    "\n",
    "Suppose $a \\geq b, b \\geq 0$:\n",
    "\n",
    "$$diff = log(exp(a) + exp(b)) - a >= log(exp(a) + 1) - a$$\n",
    "\n",
    "(3)\n",
    "\n",
    "Suppose $a \\geq b$, then \\lambda a \\geq \\lambda b$:\n",
    "\n",
    "$$\\lambda^{-1} log(exp(\\lambda a)+exp(\\lambda b)) \\geq \\lambda^{-1} log(exp( \\lambda a)) = a = max(a, b)$$\n",
    "\n",
    "(4)\n",
    "Suppose $a \\geq b, b = 0$:\n",
    "\n",
    "$$diff = \\lambda^{-1} log(exp(\\lambda a)+exp(\\lambda b)) - a =\\lambda^{-1} log (exp(\\lambda a)+ 1) - a$$\n",
    "\n",
    "$$\\lim_{\\lambda \\to \\infty} diff = \\lim_{\\lambda \\to \\infty} \\frac{log(1+exp(-\\lambda a))}{\\lambda}=0 $$\n",
    "\n",
    "(5)\n",
    "\n",
    "$$RealSoftMin(a, b) = -log(exp(-a)+exp(-b))$$\n",
    "\n",
    "(6)\n",
    "\n",
    "$$RealSoftMax(a_1, a_2, ... a_n) =log( \\sum_{i=0}^n exp(a_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The function $g(\\mathbf{x}) \\stackrel{\\textrm{def}}{=} \\log \\sum_i \\exp x_i$ is sometimes also referred to as the [log-partition function](https://en.wikipedia.org/wiki/Partition_function_(mathematics)).\n",
    "    1. Prove that the function is convex. Hint: to do so, use the fact that the first derivative amounts to the probabilities from the softmax function and show that the second derivative is the variance.\n",
    "    2. Show that $g$ is translation invariant, i.e., $g(\\mathbf{x} + b) = g(\\mathbf{x})$.\n",
    "    3. What happens if some of the coordinates $x_i$ are very large? What happens if they're all very small?\n",
    "    4. Show that if we choose $b = \\mathrm{max}_i x_i$ we end up with a numerically stable implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "\n",
    "$$\\partial_{x_i} g(\\mathbf{x}) = \\frac{exp(x_i)}{\\sum_i exp (x_i)}$$\n",
    "\n",
    "so, $g'(\\mathbf{x}) = softmax(\\mathbf{x})$\n",
    "\n",
    "$$\\frac{\\partial^2 g(\\mathbf{x})}{\\partial x_i^2} = \\frac{exp(x_i)\\sum_i exp (x_i) - (exp(x_i))^2}{(\\sum_i exp (x_i))^2} = softmax(\\mathbf{x})_i( 1 - softmax(\\mathbf{x})_i)$$\n",
    "\n",
    "so, $g''(x) > 0$ . The function is convex.\n",
    "\n",
    "(2)\n",
    "\n",
    "$$g(x+b) = log \\sum_i exp(x_i + b) = log \\sum_i exp(x_i) exp(b) = log exp(b) \\sum_i exp(x) = g(x) + b$$\n",
    "\n",
    "(3)\n",
    "\n",
    "If some $x_i$ are very large, the corresponding $exp(x_i)$ terms might be larger than the largest number we can have for certain data types. This can lead to overflow.\n",
    "\n",
    "If all $x_i$ are very large negative numbers, the corresponding $exp(x_i)$ terms become close to zero. If it is less than the smallest positive number that can be represented by a data type, it will cause underflow. The denominator will become 0.\n",
    "\n",
    "(4)\n",
    "\n",
    "$$g(x) = g(x -  max_i x_i) + max_i x_i$$\n",
    "\n",
    "This form ensures that the largest value, b, is subtracted from all $x_i$, reducing the potential for numerical instability due to large exponentials. This is often used in practice to improve the numerical stability of computing g(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Assume that we have some probability distribution $P$. Suppose we pick another distribution $Q$ with $Q(i) \\propto P(i)^\\alpha$ for $\\alpha > 0$.\n",
    "    1. Which choice of $\\alpha$ corresponds to doubling the temperature? Which choice corresponds to halving it?\n",
    "    2. What happens if we let the temperature approach $0$?\n",
    "    3. What happens if we let the temperature approach $\\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "$$P(i) = \\frac{exp(x_i / T_p)}{\\sum_i exp(x_i / T_p)}, Q(i) = \\frac{exp(x_i / T_q)}{\\sum_i exp(x_i / T_q)}$$\n",
    "\n",
    "$$\\frac{Q(i)}{Q(j)} = \\left(\\frac{P(i)}{P(j)}\\right)^\\alpha $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$exp(\\frac{x_i-x_j}{T_q})=exp(\\frac{\\alpha(x_i-x_j)}{T_p})$$\n",
    "$$\\frac{T_p}{T_q} = \\alpha$$\n",
    "\n",
    "$$T_q = 2 T_p: \\alpha = 1/2$$\n",
    "$$T_q = T_p / 2: \\alpha = 2$$\n",
    "\n",
    "(2)\n",
    "\n",
    "$$P(i) = \\frac{exp(x_i / T)}{\\sum_i exp(x_i / T)}$$\n",
    "\n",
    "If $T \\to 0$, the exponential term with the largest value will dominate the denominator, and all other terms will approach 0. The softmax function will assign all probability mass to the maximum element and zero probability to the other elements. \n",
    "\n",
    "(3)\n",
    "\n",
    "If $T \\to \\infty$, $x_i / T \\to 0$, $exp(x_i / T) \\to 1$: $P(i) \\to \\frac{1}{n}$ . So it will become a uniform distribution.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
