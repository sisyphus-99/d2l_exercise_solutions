{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3f2bac-3d3e-40c9-b9c0-0380fabc464c",
   "metadata": {},
   "source": [
    "1. **Assume that we have some data $x_1, \\ldots, x_n \\in \\mathbb{R}$. Our goal is to find a constant $b$ such that $\\sum_i (x_i - b)^2$ is minimized.**\n",
    "    1. Find an analytic solution for the optimal value of $b$.\n",
    "    1. How does this problem and its solution relate to the normal distribution?\n",
    "    1. What if we change the loss from $\\sum_i (x_i - b)^2$ to $\\sum_i |x_i-b|$? Can you find the optimal solution for $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03022618-13fe-4558-8aad-fa7b4a804ed8",
   "metadata": {},
   "source": [
    "A.\n",
    "$$\\sum_i (x_i-b)^2 = \\sum_i x_i^2 - 2b\\sum_i x_i + nb^2=n(b-\\frac{1}{n}\\sum_i x_i)^2+\\sum_i x_i^2-\\frac{(\\sum_i x_i)^2}{n}$$\n",
    "Let $b = \\frac{1}{n}\\sum_i x_i$, the function gets the minimum value.\n",
    "\n",
    "B.\n",
    "Assume that $X = b + \\epsilon$ where the noise ùúñ is normally distributed, if we want to estimate b, then we will minimize $\\sum_i (x_i - b)^2$ and the solution is the mean of all samples.\n",
    "\n",
    "C.\n",
    "From the [link](https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-ell-1-norm)\n",
    "\n",
    "![3_1_1](material/3_1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88c522-822a-456d-9095-0259ecc233dc",
   "metadata": {},
   "source": [
    "2. **Prove that the affine functions that can be expressed by $\\mathbf{x}^\\top \\mathbf{w} + b$ are equivalent to linear functions on $(\\mathbf{x}, 1)$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592a444-09ab-402d-8987-7fc83e75ed0d",
   "metadata": {},
   "source": [
    "$$\\mathbf{x}^T\\mathbf{W}+b=x_1*w_1 + ... + x_n * w_n + 1 * b = (\\mathbf{x},1)^T(\\mathbf{w},b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897e74a-0b5d-40f5-8cee-08fbf69ef003",
   "metadata": {},
   "source": [
    "3. **Assume that you want to find quadratic functions of $\\mathbf{x}$, i.e., $f(\\mathbf{x}) = b + \\sum_i w_i x_i + \\sum_{j \\leq i} w_{ij} x_{i} x_{j}$. How would you formulate this in a deep network?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd28451-480f-4051-bff0-57584a5fafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def mul(x):\n",
    "    n = x.size(0)\n",
    "    res = torch.zeros(int(n * (n + 1) / 2))\n",
    "    index = 0\n",
    "    for i in range(0, n):\n",
    "        for j in range(i, n):\n",
    "            res[index] = x[i] * x[j]\n",
    "            index += 1\n",
    "    return res\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n): \n",
    "        super(Model,self).__init__()\n",
    "        self.fc1 = nn.Linear(n, 1)\n",
    "        self.fc2 = nn.Linear(int(n * (n + 1) / 2), 1)\n",
    "    def forward(self, x):\n",
    "        y1 = self.fc1(x)\n",
    "        y2 =mul(x)\n",
    "        y2 = self.fc2(y2)\n",
    "        y = y1 + y2\n",
    "        return y\n",
    "\n",
    "model = Model(10)\n",
    "x = torch.randn(10)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03627e-eae2-4986-b87b-1fb9c44c0277",
   "metadata": {},
   "source": [
    "4. **Recall that one of the conditions for the linear regression problem to be solvable was that the design matrix $\\mathbf{X}^\\top \\mathbf{X}$ has full rank.**\n",
    "    1. What happens if this is not the case?\n",
    "    1. How could you fix it? What happens if you add a small amount of coordinate-wise independent Gaussian noise to all entries of $\\mathbf{X}$?\n",
    "    1. What is the expected value of the design matrix $\\mathbf{X}^\\top \\mathbf{X}$ in this case?\n",
    "    1. What happens with stochastic gradient descent when $\\mathbf{X}^\\top \\mathbf{X}$ does not have full rank?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48697e7b-67c3-4f75-87ab-1431974fb709",
   "metadata": {},
   "source": [
    "<font color = red>(uncertain)</font>\n",
    "\n",
    "A. We can not use the $(X^TX)^{-1}$ in the solution and there will be more than one optimal solution for (w,b)\n",
    "\n",
    "B. We can drop out some dependent dimensions and reduce the number of dimensions of both x and w, called reparameterization. If we add noise to X, the matrix $X^TX$ will have full rank.\n",
    "\n",
    "C. \n",
    "$$E((X+\\epsilon)^T(X+\\epsilon))=E(X^TX)+E(X^T\\epsilon)+E(\\epsilon^TX)+E(\\epsilon^T\\epsilon)=E(X^TX)$$\n",
    "\n",
    "D. Stochastic gradient descent (SGD) can still be used when (\\mathbf{X}^\\top \\mathbf{X}) does not have full rank, but it may not converge to a unique solution. This is because SGD does not rely on the invertibility of (\\mathbf{X}^\\top \\mathbf{X}), but instead iteratively updates the regression coefficients based on a randomly selected subset of the data. However, the presence of linearly dependent columns in (\\mathbf{X}) can cause the loss surface to have multiple minima, which means that SGD may converge to different solutions depending on the initial values of the regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eebed3-2b84-4212-8753-62fc3e616fc0",
   "metadata": {},
   "source": [
    "5. **Assume that the noise model governing the additive noise $\\epsilon$ is the exponential distribution. That is, $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$.**\n",
    "    1. Write out the negative log-likelihood of the data under the model $-\\log P(\\mathbf y \\mid \\mathbf X)$.\n",
    "    1. Can you find a closed form solution?\n",
    "    1. Suggest a minibatch stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint: what happens near the stationary point as we keep on updating the parameters)? Can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981db36-e168-4fd9-af73-ffab8c93bf7a",
   "metadata": {},
   "source": [
    "A.\n",
    "$$P(\\mathbf{y}|\\mathbf{X})=P(\\epsilon=y-Xw)=\\prod \\frac{1}{2}exp(-|y_i-x_i^Tw|)$$\n",
    "$$L = -logP(\\mathbf{y}|\\mathbf{X})=Nlog2+\\sum_i \\|y_i-x_i^Tw\\|$$\n",
    "\n",
    "B.\n",
    "$$\\bigtriangledown_w(L)=-\\sum sign(y_i-xi^Tw)x_i=0$$\n",
    "Can't find the closed form solution.\n",
    "\n",
    "C.\n",
    "\n",
    "The gradient is therefore not a smooth function and has jumps where $sign(y_i-xi^Tw)$ changes its sign. So the function will be hard to converge. We can use a larger batchsize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbae87b-bf8c-4694-b9d9-1136277b6d1a",
   "metadata": {},
   "source": [
    "6. **Assume that we want to design a neural network with two layers by composing two linear layers. That is, the output of the first layer becomes the input of the second layer. Why would such a naive composition not work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e83ca3-eeb3-494f-bc02-4d9827197729",
   "metadata": {},
   "source": [
    "The composition of two linear layers in a neural network essentially results in one linear layer. This is due to the property of linearity: the composition of two linear functions is another linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55075199-869b-4f6f-8d79-f505e557d91f",
   "metadata": {},
   "source": [
    "7. **What happens if you want to use regression for realistic price estimation of houses or stock prices?**\n",
    "    1. Show that the additive Gaussian noise assumption is not appropriate. Hint: can we have negative prices? What about fluctuations?\n",
    "    1. Why would regression to the logarithm of the price be much better, i.e., $y = \\log \\textrm{price}$?\n",
    "    1. What do you need to worry about when dealing with pennystock, i.e., stock with very low prices? Hint: can you trade at all possible prices? Why is this a bigger problem for cheap stock? For more information review the celebrated Black--Scholes model for option pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cf18f-3dff-40d2-b29e-65d71fd28b66",
   "metadata": {},
   "source": [
    "<font color = green>(from the discussion)</font>\n",
    "\n",
    "A. Prices can't be negative but the gaussian noise allows for negative values. Stock price fluctuations are related to its previous history.\n",
    "\n",
    "B. Now the range of y is the entire real number domain.\n",
    "\n",
    "C. If the price takes a very penny value, the logarithm of it will go to a very large negative number, that makes the value changes intensely. The prices of these stocks may not be able to change smoothly due to the minimum tick size, or the smallest increment by which the price can change. This can lead to a discontinuous distribution of price changes, which is not well modeled by a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fad2cd-ae7f-46cb-97c3-4ba1eb8540d4",
   "metadata": {},
   "source": [
    "8. **Suppose we want to use regression to estimate the *number* of apples sold in a grocery store.**\n",
    "    1. What are the problems with a Gaussian additive noise model? Hint: you are selling apples, not oil.\n",
    "    1. The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) captures distributions over counts. It is given by $p(k \\mid \\lambda) = \\lambda^k e^{-\\lambda}/k!$. Here $\\lambda$ is the rate function and $k$ is the number of events you see. Prove that $\\lambda$ is the expected value of counts $k$.\n",
    "    1. Design a loss function associated with the Poisson distribution.\n",
    "    1. Design a loss function for estimating $\\log \\lambda$ instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1c98b-8a9c-4ffc-aef4-03fa0726ffd5",
   "metadata": {},
   "source": [
    "A.\n",
    "The number of apple is an integer, so it is discrete. But the Gaussian additive noise is continuous.\n",
    "\n",
    "B.\n",
    "\n",
    "$$E(k)=\\sum_{k=1}^\\infty k \\frac{\\lambda^k e^{-\\lambda}}{k!}=\\lambda e^{-\\lambda}\\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k-1)!}=\\lambda e^{-\\lambda}\\sum_{k=0}^\\infty \\frac{\\lambda^{k}}{(k)!}=\\lambda e^{-\\lambda} e^{\\lambda}=\\lambda$$\n",
    "\n",
    "C.\n",
    "$$-log(P(K|\\lambda))=-\\sum_i (k_ilog\\lambda -\\lambda - log(k_i!))= n\\lambda - log\\lambda \\sum_i k_i + \\sum_i (log(k_i!))$$\n",
    "The loss function for $\\lambda$:\n",
    "$$L(\\lambda)=n\\lambda - log\\lambda \\sum_i k_i$$\n",
    "\n",
    "D. Let $t = log\\lambda$\n",
    "$$L(t)=ne^t - t \\sum_i k_i$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
